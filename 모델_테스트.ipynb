{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!git clone https://github.com/Team-M1/badwords-classifier-train\r\n",
    "%cd badwords-classifier-train\r\n",
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 모델 불러오기\r\n",
    "\r\n",
    "각자 알아서 불러올 것"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch\r\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\r\n",
    "\r\n",
    "\r\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"대충 모델\")\r\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"대충 토크나이저\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'ElectraTokenizer'. \n",
      "The class this function is called from is 'KoCharElectraTokenizer'.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "\r\n",
    "\r\n",
    "test_df = pd.read_csv(\"data/test.csv\")\r\n",
    "test_data = test_df[\"content\"]\r\n",
    "test_label = test_df[\"labels\"]\r\n",
    "\r\n",
    "speed_test_data = test_data.head(100)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "model.to(\"cpu\")\r\n",
    "model.eval()\r\n",
    "print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 스피드 테스트"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# test\r\n",
    "%%timeit\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    tokens = tokenizer(list(speed_test_data), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\r\n",
    "    _ = model(**tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## F1 스코어"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# f1\r\n",
    "model.cuda()\r\n",
    "\r\n",
    "with torch.no_grad():\r\n",
    "    tokens = tokenizer(list(test_data), padding=\"max_length\", truncation=True, return_tensors=\"pt\")\r\n",
    "    tokens = tokens.to(\"cuda\")\r\n",
    "    preds = model(**tokens)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "\r\n",
    "logits = preds.logits.cpu().numpy()\r\n",
    "pred = np.argmax(logits, 1)\r\n",
    "\r\n",
    "f1 = f1_score(test_label, pred, average=\"macro\")\r\n",
    "print(f1)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit (conda)"
  },
  "interpreter": {
   "hash": "c86e0eb5395ede85b9f59b6e8263bc6c22037c4e880f7255165769e612363282"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}